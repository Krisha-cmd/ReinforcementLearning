# -*- coding: utf-8 -*-
"""CartPolev1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1js6gVboW3GLwjy_vE57dirpywwk7zYDj

# CartPol Environment to balance a stick
"""

# !pip install stable-baselines3[extra]

import os
import gymnasium as gym
from stable_baselines3 import PPO #PPO is the algorithm
from stable_baselines3.common.vec_env import DummyVecEnv #To vectorize the environments
from stable_baselines3.common.evaluation import evaluate_policy #evaluating the model

"""# Load Environment"""

env_name='CartPole-v1' #prebuilt environment
env=gym.make(env_name)

"""## Understanding Environment"""

print(env.reset())

print(env.action_space)
print(env.observation_space) #cart position, cart velocity, pole angle, pole angular velocity

print(env.action_space.sample() ) #every environment has an action space and an observation space
print(env.observation_space.sample())

episodes=10 #testing the environment 5 times (for cartpole each episode is 200 frames)
for episode in range(episodes):
  state=env.reset() #initial state of the environment and set of observations
  done=False
  score=0

  while not done:
    env.render() #view the graphical representaion of the environment
    action=env.action_space.sample() #random action from the sample action space
    n_state,reward,done,info=env.step(action) #getting the new environment variables by passing the action into the environment, next state, reward, is it done?
    score+=reward
    print('Episode{} Score{}'.format(episode+1,score))

"""## Training
 Model based or Model free?

Model-free: only uses the current state values to make a decision
A2C, DDPG, DQN, HER, PPO, SAC, TD3

Model-based: makes a prediction about the future state of a model to make the best decision for the desired outcome

 (Model-free algorithm:PPO stable baselines only deals with model-free
 RL lib can be used for model-based)

 spinningup.openai.com


Algorithm choice depends on:
1. Action space
"""

#logging and saving logs
#log_path=os.path.join('training','logs')
model=PPO('MlpPolicy',env,verbose=1,tensorboard_log="logs")

env=DummyVecEnv([lambda:env]) #part of environment creation

model=PPO('MlpPolicy',env,verbose=1) #defining the model, multilayer perceptron policy, env

model.learn(total_timesteps=20000)

model.save("ppo_path")

del model

model=PPO.load("ppo_path",env)

"""# Evaluation

Training Metrics depend on the algorithm

- Evaluation metrics
    - Episode Length Mean
    - Reward Mean

- Time Metrics
    - fps
    - iterations
    - time elapsed
    - total_timesteps

- Loss Metrics
    - Entropy_loss
    - Policy loss
    - Value Loss

- Other metrics
    - Explained variance
    - Learning rate
    - n_updates
"""

evaluate_policy(model,env,n_eval_episodes=10,render=True)

env.close()

"""# Testing and deploying

Time to use the model to take decisions
so we predict from the observations and choose the best possible action based on the prediction from the observation

obs=env.reset

model.predict(obs)

"""

#action, _=model.predict(obs)
env=gym.make('CartPole-v1')
env=DummyVecEnv([lambda:env])
obs=env.reset() #initial observations
model.predict(obs) #gives a prediction of action (not next state)

from google.colab import drive
drive.mount('/content/drive')

episodes=10
for episode in range(episodes):
  obs=env.reset()
  done=False
  score=0

  while not done:
    env.render()
    action,_=model.predict(obs)
    obs,reward,done,info=env.step(action)
    score+=reward
    print('Episode{} Score{}'.format(episode+1,score))